{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install these, run `pip install numpy, sklearn, matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['target']) # this is how many examples we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = data['data']\n",
    "labels = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images are 8x8 flattened into 64\n",
    "image_data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a230937b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how to see images\n",
    "im_demo = image_data[0].reshape(8,8)\n",
    "plt.imshow(im_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0] # as you can see, this is marked as a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_demo # what the array looks like in number form. 8x8 is easy to see, but large images it's hard to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the example I showed at our meeting, we want anything with a non-zero intensity to be marked as a 1. Let's do that with this simple syntax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_demo = (im_demo > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_demo # woaaah, kinda cool right. and a one-liner :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply this technique to all images\n",
    "image_data = (image_data > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 1, 0, 0],\n",
       "       [0, 0, 1, ..., 1, 0, 0],\n",
       "       [0, 0, 1, ..., 1, 1, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data # note this is a nested array, so each index is it's own image (flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "In machine learning, we make training datasets to train the data and testing datasets to test the model. This prevents what's called `overfitting`, where you over-train on the training data and you make models that don't generalize to the real world data. In this case, we are going to randomly choose 80% of the data to train on, and \"hold out\" 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1437"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = int(0.8*len(image_data)); n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1635,  603, 1184, ..., 1068, 1513,  612])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices = np.random.choice(np.arange(len(labels)), n_train, replace=False); train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = image_data[train_indices]\n",
    "train_labels = labels[train_indices]\n",
    "\n",
    "test_images = np.delete(image_data, train_indices, axis=0) # this grabs everything NOT in the train indices\n",
    "test_labels = np.delete(labels, train_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437,), (360,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to do Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index x grabs the array of length 64 where each index y is p(pixel_at_y | x)\n",
    "label_to_distribution = np.zeros((10,64))\n",
    "# probability of each \n",
    "p_labels = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup zip in python, it's cool\n",
    "for image, label in zip(train_images, train_labels):\n",
    "    # if this syntax confuses you, read: https://numpy.org/devdocs/user/basics.indexing.html\n",
    "    # it is long but this kind of syntax has come to dominate machine learning\n",
    "    # and is used in Tensorflow and PyTorch\n",
    "    # it is seriously god-tier once you learn it\n",
    "    label_to_distribution[label,:] += image\n",
    "    p_labels[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   2., 131., 148., 147., 113.,   2.,   0.,   0.,  49., 147.,\n",
       "        148., 147., 148.,  49.,   0.,   0., 124., 148., 120.,  76., 147.,\n",
       "        120.,   0.,   0., 142., 148.,  55.,  12., 147., 145.,   0.,   0.,\n",
       "        145., 148.,  30.,   4., 148., 143.,   0.,   0., 132., 148.,  61.,\n",
       "         58., 148., 133.,   0.,   0.,  51., 148., 148., 144., 148.,  92.,\n",
       "          0.,   0.,   1., 134., 148., 148., 132.,  18.,   0.],\n",
       "       [  0.,   1.,  74., 122., 141.,  99.,  39.,   0.,   0.,   9.,  91.,\n",
       "        143., 144., 118.,  34.,   0.,   1.,  33., 114., 148., 149., 111.,\n",
       "         24.,   0.,   1.,  59., 126., 144., 148., 105.,  17.,   0.,   0.,\n",
       "         44., 113., 133., 149., 101.,  13.,   0.,   0.,  24.,  80., 122.,\n",
       "        148., 103.,  16.,   0.,   0.,   6.,  94., 127., 149., 110.,  48.,\n",
       "         22.,   0.,   1.,  77., 121., 149., 124.,  58.,  23.],\n",
       "       [  0.,  59., 136., 143., 141.,  66.,   7.,   0.,   1., 113., 140.,\n",
       "        145., 144.,  95.,  14.,   0.,   1.,  94., 124., 110., 141., 104.,\n",
       "         15.,   0.,   0.,  31.,  58., 102., 142.,  87.,   9.,   0.,   0.,\n",
       "          5.,  50., 124., 141.,  53.,   3.,   0.,   0.,  25.,  96., 141.,\n",
       "        120.,  55.,  32.,   1.,   1.,  52., 142., 145., 144., 145., 137.,\n",
       "         36.,   1.,  52., 135., 143., 143., 142., 127.,  47.],\n",
       "       [  0.,  58., 144., 146., 146., 135.,  31.,   1.,   1., 104., 146.,\n",
       "        146., 145., 141.,  57.,   2.,   0.,  60.,  80.,  99., 146., 133.,\n",
       "         36.,   0.,   0.,   9.,  51., 135., 145., 129.,   7.,   0.,   0.,\n",
       "          4.,  36., 113., 145., 139.,  85.,   0.,   0.,  14.,  31.,  32.,\n",
       "         93., 144., 115.,   0.,   0.,  37., 131., 139., 139., 143., 122.,\n",
       "          4.,   0.,  35., 142., 146., 146., 138.,  56.,   2.],\n",
       "       [  0.,   0.,  22., 131., 135.,  60.,   7.,   3.,   0.,   3.,  92.,\n",
       "        138., 135.,  64.,  33.,  11.,   0.,  36., 134., 138., 102., 110.,\n",
       "         68.,  17.,   0., 118., 139., 127., 118., 133., 101.,   3.,   0.,\n",
       "        133., 138., 128., 136., 136., 123.,   0.,   6., 106., 125., 132.,\n",
       "        139., 132.,  74.,   0.,   2.,  34.,  56., 115., 137.,  83.,   4.,\n",
       "          0.,   0.,   2.,  17., 124., 135.,  57.,   0.,   0.],\n",
       "       [  0.,  58., 153., 154., 153., 151., 109.,   4.,   1., 111., 154.,\n",
       "        154., 150., 141.,  86.,   3.,   0., 132., 154., 122.,  60.,  23.,\n",
       "          3.,   0.,   0., 134., 153., 150., 137.,  88.,  22.,   0.,   0.,\n",
       "         78., 129., 126., 132., 113.,  54.,   0.,   0.,  18.,  37.,  65.,\n",
       "        111., 123.,  59.,   0.,   0.,  49., 132., 144., 148., 121.,  46.,\n",
       "          0.,   0.,  57., 152., 154., 143.,  80.,  11.,   0.],\n",
       "       [  0.,   0.,  70., 143., 135.,  59.,   2.,   0.,   0.,   5., 140.,\n",
       "        144., 118.,  34.,   2.,   0.,   0.,  47., 143., 140.,  49.,   5.,\n",
       "          1.,   0.,   0.,  88., 144., 121.,  80.,  59.,  11.,   0.,   0.,\n",
       "        112., 144., 143., 141., 134.,  96.,   0.,   0.,  97., 144., 141.,\n",
       "        106., 138., 136.,  16.,   0.,  20., 144., 144., 131., 144., 138.,\n",
       "         24.,   0.,   0.,  95., 143., 144., 143., 119.,   9.],\n",
       "       [  0.,  12., 131., 143., 143., 134.,  85.,  28.,   0.,  39., 141.,\n",
       "        142., 142., 138.,  89.,  20.,   0.,  31.,  82.,  67., 113., 138.,\n",
       "         74.,   8.,   0.,  32., 107., 122., 140., 142., 110.,   0.,   0.,\n",
       "         60., 128., 136., 143., 139., 121.,   0.,   0.,  36., 107., 141.,\n",
       "        139.,  80.,  40.,   0.,   0.,   8.,  85., 141., 114.,   8.,   0.,\n",
       "          0.,   0.,   5., 120., 141.,  71.,   2.,   0.,   0.],\n",
       "       [  0.,  13., 115., 132., 130., 113.,  29.,   0.,   1.,  71., 130.,\n",
       "        130., 128., 126.,  70.,   0.,   0.,  72., 130., 115., 115., 130.,\n",
       "         73.,   0.,   0.,  49., 121., 131., 132., 116.,  27.,   0.,   0.,\n",
       "         25., 122., 132., 129.,  85.,   7.,   0.,   0.,  47., 125., 122.,\n",
       "        125., 106.,  41.,   0.,   0.,  39., 127., 125., 123., 116.,  54.,\n",
       "          1.,   0.,  15., 109., 132., 132., 111.,  34.,   0.],\n",
       "       [  0.,  18., 115., 133., 137., 113.,  31.,   4.,   0.,  78., 133.,\n",
       "        137., 136., 136.,  56.,   4.,   0.,  90., 136., 103., 127., 136.,\n",
       "         87.,   4.,   0.,  76., 134., 137., 136., 136., 109.,   0.,   0.,\n",
       "         14.,  70.,  88.,  94., 134., 110.,   0.,   0.,   9.,  20.,  17.,\n",
       "         50., 133., 106.,   1.,   0.,  30.,  99., 103., 111., 123.,  94.,\n",
       "          6.,   0.,  11., 113., 126., 136., 121.,  57.,   4.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we counted the instances of 1 across all images for each digit\n",
    "label_to_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([148., 149., 145., 146., 139., 154., 144., 143., 132., 137.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we sum the entire array by 1 to get rid of zeros\n",
    "# zeros cause our probabilities to go to 0\n",
    "# because p(0)*p(1)...p(8) = 0 if any p_i = 0\n",
    "label_to_distribution += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((3,2)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3]) + [1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to get the PROBABLITY, so we divide each element in a row by the sum of the row\n",
    "# if this confuses you, which it should, look at each part, starting with label_to_distribution.sum(axis=1)\n",
    "# the key is understanding how broadcasting works in numpy: https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "label_to_distribution = (label_to_distribution.T / label_to_distribution.sum(axis=1)).T\n",
    "p_labels /= p_labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00018907, 0.00056721, 0.02495746, 0.02817168, 0.02798261,\n",
       "        0.02155417, 0.00056721, 0.00018907, 0.00018907, 0.00945358,\n",
       "        0.02798261, 0.02817168, 0.02798261, 0.02817168, 0.00945358,\n",
       "        0.00018907, 0.00018907, 0.02363396, 0.02817168, 0.02287767,\n",
       "        0.01455852, 0.02798261, 0.02287767, 0.00018907, 0.00018907,\n",
       "        0.02703725, 0.02817168, 0.01058801, 0.00245793, 0.02798261,\n",
       "        0.02760446, 0.00018907, 0.00018907, 0.02760446, 0.02817168,\n",
       "        0.00586122, 0.00094536, 0.02817168, 0.02722632, 0.00018907,\n",
       "        0.00018907, 0.02514653, 0.02817168, 0.01172244, 0.01115523,\n",
       "        0.02817168, 0.0253356 , 0.00018907, 0.00018907, 0.00983173,\n",
       "        0.02817168, 0.02817168, 0.02741539, 0.02817168, 0.01758366,\n",
       "        0.00018907, 0.00018907, 0.00037814, 0.02552467, 0.02817168,\n",
       "        0.02817168, 0.02514653, 0.00359236, 0.00018907],\n",
       "       [0.00022655, 0.0004531 , 0.01699139, 0.02786588, 0.03217037,\n",
       "        0.02265519, 0.00906208, 0.00022655, 0.00022655, 0.00226552,\n",
       "        0.02084277, 0.03262347, 0.03285002, 0.02695967, 0.00792932,\n",
       "        0.00022655, 0.0004531 , 0.00770276, 0.02605347, 0.03375623,\n",
       "        0.03398278, 0.02537381, 0.0056638 , 0.00022655, 0.0004531 ,\n",
       "        0.01359311, 0.02877209, 0.03285002, 0.03375623, 0.0240145 ,\n",
       "        0.00407793, 0.00022655, 0.00022655, 0.01019483, 0.02582691,\n",
       "        0.03035795, 0.03398278, 0.02310829, 0.00317173, 0.00022655,\n",
       "        0.00022655, 0.0056638 , 0.0183507 , 0.02786588, 0.03375623,\n",
       "        0.0235614 , 0.00385138, 0.00022655, 0.00022655, 0.00158586,\n",
       "        0.02152243, 0.02899864, 0.03398278, 0.02514726, 0.01110104,\n",
       "        0.00521069, 0.00022655, 0.0004531 , 0.01767105, 0.02763933,\n",
       "        0.03398278, 0.02831899, 0.01336656, 0.00543725],\n",
       "       [0.00021169, 0.0127011 , 0.02900085, 0.03048264, 0.03005927,\n",
       "        0.0141829 , 0.00169348, 0.00021169, 0.00042337, 0.02413209,\n",
       "        0.02984759, 0.03090601, 0.03069433, 0.02032176, 0.00317528,\n",
       "        0.00021169, 0.00042337, 0.02011008, 0.02646063, 0.02349704,\n",
       "        0.03005927, 0.02222693, 0.00338696, 0.00021169, 0.00021169,\n",
       "        0.00677392, 0.01248942, 0.02180356, 0.03027096, 0.01862828,\n",
       "        0.00211685, 0.00021169, 0.00021169, 0.00127011, 0.01079594,\n",
       "        0.02646063, 0.03005927, 0.01143099, 0.00084674, 0.00021169,\n",
       "        0.00021169, 0.00550381, 0.02053345, 0.03005927, 0.02561389,\n",
       "        0.01185436, 0.00698561, 0.00042337, 0.00042337, 0.01121931,\n",
       "        0.03027096, 0.03090601, 0.03069433, 0.03090601, 0.02921253,\n",
       "        0.00783235, 0.00042337, 0.01121931, 0.02878916, 0.03048264,\n",
       "        0.03048264, 0.03027096, 0.02709568, 0.01016088],\n",
       "       [0.00020713, 0.01222038, 0.03003314, 0.03044739, 0.03044739,\n",
       "        0.02816901, 0.006628  , 0.00041425, 0.00041425, 0.02174814,\n",
       "        0.03044739, 0.03044739, 0.03024027, 0.02941176, 0.01201326,\n",
       "        0.00062138, 0.00020713, 0.01263463, 0.01677713, 0.02071251,\n",
       "        0.03044739, 0.02775476, 0.00766363, 0.00020713, 0.00020713,\n",
       "        0.00207125, 0.01077051, 0.02816901, 0.03024027, 0.02692626,\n",
       "        0.001657  , 0.00020713, 0.00020713, 0.00103563, 0.00766363,\n",
       "        0.02361226, 0.03024027, 0.02899751, 0.01781276, 0.00020713,\n",
       "        0.00020713, 0.00310688, 0.006628  , 0.00683513, 0.01946976,\n",
       "        0.03003314, 0.02402651, 0.00020713, 0.00020713, 0.00787075,\n",
       "        0.02734051, 0.02899751, 0.02899751, 0.02982601, 0.02547639,\n",
       "        0.00103563, 0.00020713, 0.0074565 , 0.02961889, 0.03044739,\n",
       "        0.03044739, 0.02879039, 0.01180613, 0.00062138],\n",
       "       [0.00022143, 0.00022143, 0.005093  , 0.02922941, 0.03011515,\n",
       "        0.01350753, 0.00177148, 0.00088574, 0.00022143, 0.00088574,\n",
       "        0.02059345, 0.03077945, 0.03011515, 0.01439327, 0.00752879,\n",
       "        0.00265722, 0.00022143, 0.00819309, 0.02989371, 0.03077945,\n",
       "        0.02280779, 0.02457927, 0.01527901, 0.00398583, 0.00022143,\n",
       "        0.02635075, 0.03100089, 0.02834367, 0.02635075, 0.02967228,\n",
       "        0.02258636, 0.00088574, 0.00022143, 0.02967228, 0.03077945,\n",
       "        0.0285651 , 0.03033658, 0.03033658, 0.02745793, 0.00022143,\n",
       "        0.00155004, 0.02369353, 0.0279008 , 0.02945084, 0.03100089,\n",
       "        0.02945084, 0.01660762, 0.00022143, 0.0006643 , 0.00775022,\n",
       "        0.01262179, 0.02568645, 0.03055802, 0.01860053, 0.00110717,\n",
       "        0.00022143, 0.00022143, 0.0006643 , 0.00398583, 0.02767936,\n",
       "        0.03011515, 0.01284322, 0.00022143, 0.00022143],\n",
       "       [0.00019585, 0.01155503, 0.0301606 , 0.03035644, 0.0301606 ,\n",
       "        0.0297689 , 0.02154328, 0.00097924, 0.0003917 , 0.02193498,\n",
       "        0.03035644, 0.03035644, 0.02957305, 0.02781042, 0.01703878,\n",
       "        0.00078339, 0.00019585, 0.02604779, 0.03035644, 0.02408931,\n",
       "        0.01194673, 0.00470035, 0.00078339, 0.00019585, 0.00019585,\n",
       "        0.02643948, 0.0301606 , 0.02957305, 0.02702703, 0.01743047,\n",
       "        0.0045045 , 0.00019585, 0.00019585, 0.01547199, 0.02546024,\n",
       "        0.0248727 , 0.02604779, 0.02232667, 0.01077164, 0.00019585,\n",
       "        0.00019585, 0.00372111, 0.00744222, 0.01292597, 0.02193498,\n",
       "        0.02428515, 0.01175088, 0.00019585, 0.00019585, 0.0097924 ,\n",
       "        0.02604779, 0.02839796, 0.02918136, 0.02389346, 0.00920486,\n",
       "        0.00019585, 0.00019585, 0.01135919, 0.02996475, 0.03035644,\n",
       "        0.02820212, 0.01586369, 0.00235018, 0.00019585],\n",
       "       [0.00021053, 0.00021053, 0.01494737, 0.03031579, 0.02863158,\n",
       "        0.01263158, 0.00063158, 0.00021053, 0.00021053, 0.00126316,\n",
       "        0.02968421, 0.03052632, 0.02505263, 0.00736842, 0.00063158,\n",
       "        0.00021053, 0.00021053, 0.01010526, 0.03031579, 0.02968421,\n",
       "        0.01052632, 0.00126316, 0.00042105, 0.00021053, 0.00021053,\n",
       "        0.01873684, 0.03052632, 0.02568421, 0.01705263, 0.01263158,\n",
       "        0.00252632, 0.00021053, 0.00021053, 0.02378947, 0.03052632,\n",
       "        0.03031579, 0.02989474, 0.02842105, 0.02042105, 0.00021053,\n",
       "        0.00021053, 0.02063158, 0.03052632, 0.02989474, 0.02252632,\n",
       "        0.02926316, 0.02884211, 0.00357895, 0.00021053, 0.00442105,\n",
       "        0.03052632, 0.03052632, 0.02778947, 0.03052632, 0.02926316,\n",
       "        0.00526316, 0.00021053, 0.00021053, 0.02021053, 0.03031579,\n",
       "        0.03052632, 0.03031579, 0.02526316, 0.00210526],\n",
       "       [0.00021825, 0.00283719, 0.02880838, 0.03142732, 0.03142732,\n",
       "        0.02946312, 0.0187691 , 0.00632911, 0.00021825, 0.00872981,\n",
       "        0.03099083, 0.03120908, 0.03120908, 0.0303361 , 0.01964208,\n",
       "        0.00458315, 0.00021825, 0.00698385, 0.01811436, 0.01484068,\n",
       "        0.02487997, 0.0303361 , 0.0163684 , 0.00196421, 0.00021825,\n",
       "        0.0072021 , 0.02357049, 0.02684417, 0.03077259, 0.03120908,\n",
       "        0.02422523, 0.00021825, 0.00021825, 0.01331296, 0.02815364,\n",
       "        0.02989961, 0.03142732, 0.03055434, 0.02662593, 0.00021825,\n",
       "        0.00021825, 0.00807508, 0.02357049, 0.03099083, 0.03055434,\n",
       "        0.01767787, 0.00894806, 0.00021825, 0.00021825, 0.00196421,\n",
       "        0.0187691 , 0.03099083, 0.02509821, 0.00196421, 0.00021825,\n",
       "        0.00021825, 0.00021825, 0.00130947, 0.02640768, 0.03099083,\n",
       "        0.01571366, 0.00065474, 0.00021825, 0.00021825],\n",
       "       [0.00021519, 0.0030127 , 0.02496234, 0.02862062, 0.02819023,\n",
       "        0.02453196, 0.00645578, 0.00021519, 0.00043039, 0.01549387,\n",
       "        0.02819023, 0.02819023, 0.02775985, 0.02732946, 0.01527867,\n",
       "        0.00021519, 0.00021519, 0.01570906, 0.02819023, 0.02496234,\n",
       "        0.02496234, 0.02819023, 0.01592425, 0.00021519, 0.00021519,\n",
       "        0.01075963, 0.0262535 , 0.02840542, 0.02862062, 0.02517753,\n",
       "        0.00602539, 0.00021519, 0.00021519, 0.00559501, 0.02646869,\n",
       "        0.02862062, 0.02797504, 0.01850656, 0.00172154, 0.00021519,\n",
       "        0.00021519, 0.01032924, 0.02711427, 0.02646869, 0.02711427,\n",
       "        0.02302561, 0.00903809, 0.00021519, 0.00021519, 0.0086077 ,\n",
       "        0.02754465, 0.02711427, 0.02668388, 0.02517753, 0.01183559,\n",
       "        0.00043039, 0.00021519, 0.00344308, 0.02367119, 0.02862062,\n",
       "        0.02862062, 0.02410157, 0.00753174, 0.00021519],\n",
       "       [0.0002134 , 0.00405463, 0.02475459, 0.02859582, 0.02944942,\n",
       "        0.02432778, 0.00682885, 0.00106701, 0.0002134 , 0.01685873,\n",
       "        0.02859582, 0.02944942, 0.02923602, 0.02923602, 0.01216389,\n",
       "        0.00106701, 0.0002134 , 0.01941955, 0.02923602, 0.02219377,\n",
       "        0.02731541, 0.02923602, 0.01877934, 0.00106701, 0.0002134 ,\n",
       "        0.01643192, 0.02880922, 0.02944942, 0.02923602, 0.02923602,\n",
       "        0.02347418, 0.0002134 , 0.0002134 , 0.00320102, 0.01515152,\n",
       "        0.01899274, 0.02027315, 0.02880922, 0.02368758, 0.0002134 ,\n",
       "        0.0002134 , 0.00213402, 0.00448143, 0.00384123, 0.01088348,\n",
       "        0.02859582, 0.02283397, 0.0004268 , 0.0002134 , 0.00661545,\n",
       "        0.02134016, 0.02219377, 0.02390098, 0.0264618 , 0.02027315,\n",
       "        0.00149381, 0.0002134 , 0.00256082, 0.02432778, 0.02710201,\n",
       "        0.02923602, 0.026035  , 0.01237729, 0.00106701]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10299235, 0.10368824, 0.10090466, 0.10160056, 0.0967293 ,\n",
       "       0.10716771, 0.10020877, 0.09951287, 0.09185804, 0.09533751])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how our model does on the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for image, label in zip(train_images, train_labels):\n",
    "    # we grab all indices with a 1 and multiply up the probability of seeing a 1 at that pixel for each number\n",
    "    pos_indices = np.argwhere(image == 1)\n",
    "    prob_per_label = np.prod(label_to_distribution[:,pos_indices] * image[pos_indices], axis=1) # P(B|A)\n",
    "    prob_per_label *= p_labels.reshape(-1,1) # P(A)\n",
    "    # this grabs the largest index, which in this case is the label we predict!\n",
    "    pred = np.argmax(prob_per_label)\n",
    "    if pred == label:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1206, 0.8392484342379958)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1209 correct, which is REMARKABLY accurate!\n",
    "correct, correct / len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.52416881, -8.52416881, -3.6643564 , -3.57540891, -3.58252638,\n",
       "        -3.87020845, -6.91473089, -8.52416881, -8.52416881, -4.65296779,\n",
       "        -3.57540891, -3.57540891, -3.57540891, -3.57540891, -4.81059674,\n",
       "        -8.52416881, -8.52416881, -3.72014776, -3.57540891, -3.77057861,\n",
       "        -4.18036338, -3.57540891, -3.8146386 , -8.52416881, -8.52416881,\n",
       "        -3.61151392, -3.57540891, -4.46372579, -6.12627353, -3.57540891,\n",
       "        -3.59691512, -8.52416881, -8.52416881, -3.59691512, -3.57540891,\n",
       "        -5.02766124, -7.42555652, -3.57540891, -3.60418788, -8.52416881,\n",
       "        -8.52416881, -3.69585507, -3.57540891, -4.46372579, -4.55387689,\n",
       "        -3.57540891, -3.65663435, -8.52416881, -8.52416881, -4.53518476,\n",
       "        -3.57540891, -3.57540891, -3.60418788, -3.57540891, -4.00238023,\n",
       "        -8.52416881, -8.52416881, -7.83102162, -3.64136688, -3.57540891,\n",
       "        -3.57540891, -3.67998172, -5.43312635, -8.52416881],\n",
       "       [-8.40447232, -7.71132514, -4.03502447, -3.56028523, -3.44162769,\n",
       "        -3.77949951, -4.71559287, -8.40447232, -8.40447232, -6.10188723,\n",
       "        -3.79930214, -3.42773858, -3.41403973, -3.60868178, -4.76688616,\n",
       "        -8.40447232, -8.40447232, -4.82095338, -3.62534883, -3.39383703,\n",
       "        -3.38719248, -3.66827387, -5.14637578, -8.40447232, -8.40447232,\n",
       "        -4.24558924, -3.54465992, -3.4208657 , -3.39383703, -3.70399196,\n",
       "        -5.46003334, -8.40447232, -8.40447232, -4.62028269, -3.69494212,\n",
       "        -3.4844914 , -3.38719248, -3.77949951, -5.6318836 , -8.40447232,\n",
       "        -8.40447232, -5.14637578, -4.02244569, -3.58419076, -3.39383703,\n",
       "        -3.77949951, -5.51410056, -8.40447232, -8.40447232, -6.20724774,\n",
       "        -3.84012413, -3.53693787, -3.38719248, -3.70399196, -4.55432472,\n",
       "        -5.31342987, -8.40447232, -7.71132514, -4.03502447, -3.56819041,\n",
       "        -3.38719248, -3.60045128, -4.37912063, -5.26897811],\n",
       "       [-8.49841804, -4.38754417, -3.52860474, -3.49447173, -3.49447173,\n",
       "        -4.24992279, -6.70665857, -8.49841804, -8.49841804, -3.71092629,\n",
       "        -3.51481141, -3.4811382 , -3.4811382 , -3.88329752, -5.79036783,\n",
       "        -8.49841804, -7.80527086, -3.87344522, -3.61561611, -3.79793767,\n",
       "        -3.50120576, -3.79793767, -5.79036783, -8.49841804, -8.49841804,\n",
       "        -4.83485639, -4.33953495, -3.83497894, -3.49447173, -3.96581854,\n",
       "        -6.41897649, -8.49841804, -8.49841804, -6.70665857, -4.54717432,\n",
       "        -3.65423095, -3.49447173, -4.45536677, -7.11212367, -8.49841804,\n",
       "        -8.49841804, -5.4538956 , -3.86368905, -3.50120576, -3.6701043 ,\n",
       "        -4.40407347, -4.9148991 , -7.11212367, -7.80527086, -4.49108485,\n",
       "        -3.50798545, -3.4811382 , -3.48778274, -3.4811382 , -3.54259098,\n",
       "        -4.78484597, -7.80527086, -4.54717432, -3.53557341, -3.49447173,\n",
       "        -3.49447173, -3.50120576, -3.63088359, -4.54717432],\n",
       "       [-8.53306654, -4.35867927, -3.5157867 , -3.49611394, -3.49611394,\n",
       "        -3.57022191, -4.84418709, -7.83991936, -7.43445425, -3.76238192,\n",
       "        -3.49611394, -3.49611394, -3.50262862, -3.53585427, -4.32837392,\n",
       "        -7.83991936, -7.83991936, -4.50771485, -4.15103991, -3.87910619,\n",
       "        -3.50262862, -3.57022191, -4.74887691, -8.53306654, -8.53306654,\n",
       "        -6.13517127, -4.70442514, -3.57723948, -3.50918602, -3.62041165,\n",
       "        -5.96811718, -8.53306654, -8.53306654, -6.92362863, -4.9495476 ,\n",
       "        -3.77947635, -3.50262862, -3.54945992, -4.03325687, -8.53306654,\n",
       "        -8.53306654, -5.58862756, -4.86950489, -5.00670602, -3.96871835,\n",
       "        -3.50918602, -3.73727599, -8.53306654, -8.53306654, -4.72640405,\n",
       "        -3.61308561, -3.54945992, -3.54263395, -3.50918602, -3.68103628,\n",
       "        -7.14677218, -8.53306654, -4.70442514, -3.5157867 , -3.49611394,\n",
       "        -3.49611394, -3.5563328 , -4.40593216, -8.53306654],\n",
       "       [-8.43620003, -8.43620003, -5.21732421, -3.56100271, -3.5017261 ,\n",
       "        -4.27731695, -6.64444056, -7.33758774, -8.43620003, -6.64444056,\n",
       "        -3.93639036, -3.48037297, -3.51621911, -4.34185547, -5.10399552,\n",
       "        -5.95129338, -8.43620003, -4.72262797, -3.5017261 , -3.48037297,\n",
       "        -3.79180913, -3.6912679 , -4.14574059, -5.60298669, -7.74305285,\n",
       "        -3.64870829, -3.4733554 , -3.55339811, -3.66551541, -3.50894635,\n",
       "        -3.74485215, -7.04990567, -8.43620003, -3.51621911, -3.48037297,\n",
       "        -3.56100271, -3.51621911, -3.49455761, -3.59201295, -8.43620003,\n",
       "        -6.64444056, -3.71770116, -3.56866558, -3.52354515, -3.4733554 ,\n",
       "        -3.51621911, -4.07949121, -8.43620003, -7.04990567, -4.88085197,\n",
       "        -4.35866259, -3.6740261 , -3.48744014, -3.90360054, -7.04990567,\n",
       "        -8.43620003, -8.43620003, -7.33758774, -5.34515758, -3.60788629,\n",
       "        -3.50894635, -4.29306531, -8.43620003, -8.43620003],\n",
       "       [-8.54752839, -4.43665453, -3.49767238, -3.49128259, -3.50410327,\n",
       "        -3.51057579, -3.82902952, -6.75576892, -7.85438121, -3.7939382 ,\n",
       "        -3.49128259, -3.49128259, -3.51057579, -3.57771509, -4.07019158,\n",
       "        -6.93809048, -8.54752839, -3.64225361, -3.49128259, -3.67999394,\n",
       "        -4.37314112, -5.45648594, -6.93809048, -8.54752839, -8.54752839,\n",
       "        -3.64225361, -3.49767238, -3.49767238, -3.62754747, -4.09318109,\n",
       "        -5.50300595, -8.54752839, -8.54752839, -4.17808054, -3.67999394,\n",
       "        -3.67999394, -3.5987685 , -3.81132994, -4.61570276, -8.54752839,\n",
       "        -8.54752839, -5.83947819, -4.93661048, -4.29903315, -3.7684049 ,\n",
       "        -3.71921465, -4.55854434, -8.54752839, -8.54752839, -4.65570809,\n",
       "        -3.64968859, -3.54358209, -3.5368931 , -3.75173785, -4.83395632,\n",
       "        -8.54752839, -8.54752839, -4.45318383, -3.50410327, -3.49128259,\n",
       "        -3.57771509, -4.21679505, -6.14963312, -8.54752839],\n",
       "       [-8.37724123, -8.37724123, -4.21835815, -3.49443931, -3.54892749,\n",
       "        -4.35188954, -7.27862894, -8.37724123, -8.37724123, -6.76780332,\n",
       "        -3.53305414, -3.4868921 , -3.69511   , -4.79372229, -7.27862894,\n",
       "        -8.37724123, -8.37724123, -4.57057874, -3.49443931, -3.50970678,\n",
       "        -4.66366916, -6.58548176, -7.68409405, -8.37724123, -8.37724123,\n",
       "        -4.00779338, -3.49443931, -3.65874236, -4.05975312, -4.28289667,\n",
       "        -6.07465614, -8.37724123, -8.37724123, -3.79227375, -3.4868921 ,\n",
       "        -3.49443931, -3.51742883, -3.54095932, -3.84464174, -8.37724123,\n",
       "        -8.37724123, -3.88860486, -3.4868921 , -3.50970678, -3.82336434,\n",
       "        -3.52521097, -3.54892749, -5.66919103, -8.37724123, -5.54402789,\n",
       "        -3.4868921 , -3.4868921 , -3.59811774, -3.4868921 , -3.52521097,\n",
       "        -5.24174702, -8.37724123, -8.37724123, -3.91133311, -3.49443931,\n",
       "        -3.4868921 , -3.49443931, -3.66771103, -6.18001665],\n",
       "       [-8.3990851 , -6.09650001, -3.5392727 , -3.45744268, -3.45744268,\n",
       "        -3.50873597, -4.02963725, -5.30804265, -8.3990851 , -4.66141548,\n",
       "        -3.46461117, -3.45744268, -3.46461117, -3.48643022, -3.91044873,\n",
       "        -5.50871335, -8.3990851 , -4.73552346, -3.85579032, -4.32154766,\n",
       "        -3.68058623, -3.5012453 , -4.1795774 , -6.45317495, -8.3990851 ,\n",
       "        -5.03178927, -3.81411762, -3.64549491, -3.47183142, -3.46461117,\n",
       "        -3.78396459, -8.3990851 , -8.3990851 , -4.46725947, -3.55489802,\n",
       "        -3.5012453 , -3.45744268, -3.47910418, -3.61159336, -8.3990851 ,\n",
       "        -8.3990851 , -4.76149894, -3.6895549 , -3.47183142, -3.49381032,\n",
       "        -3.9682683 , -4.63788499, -8.3990851 , -8.3990851 , -6.78964719,\n",
       "        -4.02963725, -3.47183142, -3.67169728, -5.91417845, -8.3990851 ,\n",
       "        -8.3990851 , -8.3990851 , -7.01279074, -3.65415297, -3.46461117,\n",
       "        -4.12241898, -7.30047281, -8.3990851 , -8.3990851 ],\n",
       "       [-8.4661104 , -5.82705307, -3.70393647, -3.55345552, -3.57576127,\n",
       "        -3.69542578, -4.88259146, -8.4661104 , -7.77296322, -4.20343052,\n",
       "        -3.5682706 , -3.56083562, -3.57576127, -3.59091308, -4.17565096,\n",
       "        -8.4661104 , -7.77296322, -4.16204531, -3.57576127, -3.70393647,\n",
       "        -3.65392605, -3.5682706 , -4.14862229, -8.4661104 , -8.4661104 ,\n",
       "        -4.49581849, -3.66208936, -3.56083562, -3.55345552, -3.67861866,\n",
       "        -5.24723458, -8.4661104 , -8.4661104 , -5.13390589, -3.62982849,\n",
       "        -3.55345552, -3.57576127, -4.04726979, -6.26888582, -8.4661104 ,\n",
       "        -8.4661104 , -4.53428477, -3.62982849, -3.62192331, -3.59857595,\n",
       "        -3.7565802 , -4.72844078, -8.4661104 , -8.4661104 , -4.72844078,\n",
       "        -3.606298  , -3.62192331, -3.62982849, -3.69542578, -4.42305913,\n",
       "        -7.77296322, -8.4661104 , -5.63289706, -3.74761153, -3.55345552,\n",
       "        -3.55345552, -3.73872258, -4.82852424, -7.77296322],\n",
       "       [-8.48115142, -5.64793808, -3.69365968, -3.55389773, -3.53239153,\n",
       "        -3.72756123, -5.01541552, -6.87171351, -8.48115142, -4.03850016,\n",
       "        -3.55389773, -3.53239153, -3.54667749, -3.54667749, -4.35401704,\n",
       "        -6.87171351, -8.48115142, -3.88603157, -3.539509  , -3.85617861,\n",
       "        -3.5983495 , -3.539509  , -3.89618394, -7.38253913, -8.48115142,\n",
       "        -4.08670227, -3.55389773, -3.53239153, -3.53239153, -3.53239153,\n",
       "        -3.73621929, -8.48115142, -8.48115142, -5.91620206, -4.2044853 ,\n",
       "        -3.95936284, -3.90644044, -3.55389773, -3.76265255, -8.48115142,\n",
       "        -8.48115142, -6.40170988, -5.39010897, -5.30309759, -4.42070841,\n",
       "        -3.56117049, -3.78980354, -7.78800424, -8.48115142, -4.89763248,\n",
       "        -3.89618394, -3.78067105, -3.69365968, -3.62912116, -3.93785664,\n",
       "        -6.53524127, -8.48115142, -5.91620206, -3.7104668 , -3.6059541 ,\n",
       "        -3.55389773, -3.66086985, -4.43810015, -6.68939195]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the log stuff I was discussing\n",
    "log_distribution = np.log(label_to_distribution)\n",
    "log_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for image, label in zip(train_images, train_labels):\n",
    "    prob_per_label = np.sum(log_distribution * image, axis=1)\n",
    "    prob_per_label *= p_labels\n",
    "    pred = np.argmax(prob_per_label)\n",
    "    if pred == label:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(604, 0.42032011134307584)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# less correct? But didn't Jatin say these should be the same... is he a liar...\n",
    "# I'll leave this in to maybe think about later, but perhaps you guys can help\n",
    "# me figure out what I did wrong\n",
    "correct, correct / len(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for image, label in zip(test_images, test_labels):\n",
    "    pos_indices = np.argwhere(image == 1)\n",
    "    prob_per_label = np.prod(label_to_distribution[:,pos_indices] * image[pos_indices], axis=1) # P(B|A)\n",
    "    prob_per_label *= p_labels.reshape(-1,1) # P(A)\n",
    "    # this grabs the largest index, which in this case is the label we predict!\n",
    "    pred = np.argmax(prob_per_label)\n",
    "    if pred == label:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297, 0.825)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seems like we did just as good\n",
    "correct, correct/len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
